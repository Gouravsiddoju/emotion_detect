{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f80a27-3cd7-4d3c-91b8-80d05b48095b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 2s/step - loss: 0.1952 - mae: 0.3539 - val_loss: 0.1803 - val_mae: 0.3570 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 2s/step - loss: 0.1813 - mae: 0.3585 - val_loss: 0.1799 - val_mae: 0.3599 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 4s/step - loss: 0.1805 - mae: 0.3599 - val_loss: 0.1795 - val_mae: 0.3569 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 3s/step - loss: 0.1791 - mae: 0.3587 - val_loss: 0.1782 - val_mae: 0.3578 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - loss: 0.1791 - mae: 0.3584 - val_loss: 0.1771 - val_mae: 0.3540 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 2s/step - loss: 0.1749 - mae: 0.3520 - val_loss: 0.1715 - val_mae: 0.3448 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 2s/step - loss: 0.1694 - mae: 0.3427 - val_loss: 0.1671 - val_mae: 0.3323 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 2s/step - loss: 0.1612 - mae: 0.3302 - val_loss: 0.1590 - val_mae: 0.3293 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 2s/step - loss: 0.1500 - mae: 0.3148 - val_loss: 0.1309 - val_mae: 0.2933 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 2s/step - loss: 0.1289 - mae: 0.2904 - val_loss: 0.1362 - val_mae: 0.2908 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 2s/step - loss: 0.1121 - mae: 0.2635 - val_loss: 0.0745 - val_mae: 0.2068 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 2s/step - loss: 0.0584 - mae: 0.1807 - val_loss: 0.0489 - val_mae: 0.1625 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 2s/step - loss: 0.0342 - mae: 0.1316 - val_loss: 0.0379 - val_mae: 0.1355 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 2s/step - loss: 0.0463 - mae: 0.1457 - val_loss: 0.0199 - val_mae: 0.0825 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - loss: 0.0112 - mae: 0.0648 - val_loss: 0.0137 - val_mae: 0.0563 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2s/step - loss: 0.0067 - mae: 0.0462 - val_loss: 0.0120 - val_mae: 0.0472 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 2s/step - loss: 0.0065 - mae: 0.0399 - val_loss: 0.0113 - val_mae: 0.0409 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 2s/step - loss: 0.0055 - mae: 0.0350 - val_loss: 0.0105 - val_mae: 0.0352 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 2s/step - loss: 0.0063 - mae: 0.0317 - val_loss: 0.0103 - val_mae: 0.0330 - learning_rate: 1.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 2s/step - loss: 0.0039 - mae: 0.0255 - val_loss: 0.0101 - val_mae: 0.0321 - learning_rate: 1.0000e-04\n",
      "Test Loss: 0.0061, Test MAE: 0.0277\n",
      "Sample 1 - Actual: ['scrambled' 'scrambled' 'scrambled' 'scrambled' 'scrambled'], Predicted: ['scrambled' 'scrambled' 'scrambled' 'scrambled' 'scrambled']\n",
      "Sample 2 - Actual: ['happy' 'happy' 'happy' 'happy' 'happy'], Predicted: ['happy' 'happy' 'happy' 'happy' 'happy']\n",
      "Sample 3 - Actual: ['scrambled' 'scrambled' 'scrambled' 'scrambled' 'scrambled'], Predicted: ['scrambled' 'scrambled' 'scrambled' 'scrambled' 'scrambled']\n",
      "Sample 4 - Actual: ['blank' 'blank' 'blank' 'blank' 'blank'], Predicted: ['blank' 'blank' 'blank' 'blank' 'blank']\n",
      "Sample 5 - Actual: ['scrambled' 'scrambled' 'scrambled' 'scrambled' 'scrambled'], Predicted: ['scrambled' 'scrambled' 'scrambled' 'scrambled' 'scrambled']\n",
      "Model and label encoder saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import gzip\n",
    "from skimage.transform import resize\n",
    "import pickle\n",
    "\n",
    "# Constants\n",
    "dataset_path = os.path.normpath(\"D:/PROJECTS/emotion_detect/archive\")\n",
    "label_path = os.path.join(dataset_path, \"onsetime\")\n",
    "target_shape = (32, 32, 32)\n",
    "sequence_length = 20\n",
    "future_steps = 5\n",
    "batch_size = 5\n",
    "VALID_EMOTIONS = [\"happy\", \"sad\", \"angry\", \"neutral\", \"scrambled\", \"blank\"]\n",
    "\n",
    "# Optimize GPU usage\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# Load emotion labels\n",
    "def load_labels_from_tsv(labels_dir):\n",
    "    label_mapping = {}\n",
    "    if not os.path.exists(labels_dir):\n",
    "        print(f\"Label directory '{labels_dir}' not found!\")\n",
    "        return label_mapping\n",
    "\n",
    "    for tsv_file in os.listdir(labels_dir):\n",
    "        if tsv_file.endswith(\".tsv\"):\n",
    "            subject_run = re.search(r\"task-emotionalfaces_run-(\\d+)_events\", tsv_file)\n",
    "            if subject_run:\n",
    "                df = pd.read_csv(os.path.join(labels_dir, tsv_file), sep='\\t', usecols=['trial_type'])\n",
    "                valid_trial_types = df['trial_type'].dropna()[df['trial_type'].isin(VALID_EMOTIONS)].tolist()\n",
    "                if valid_trial_types:\n",
    "                    label_mapping[subject_run.group(1)] = valid_trial_types\n",
    "    return label_mapping\n",
    "\n",
    "# Process fMRI sequences\n",
    "def process_fmri_file_sequence(fmri_file_path, label_mapping):\n",
    "    subject_run = re.search(r\"wrsub-(\\d+)_task-emotionalfaces_run-(\\d+)\", os.path.basename(fmri_file_path))\n",
    "    if not subject_run:\n",
    "        return None, None\n",
    "\n",
    "    run_id = subject_run.group(2)\n",
    "    labels = label_mapping.get(run_id)\n",
    "    if not labels:\n",
    "        return None, None\n",
    "\n",
    "    if fmri_file_path.endswith(\".nii.gz\"):\n",
    "        with gzip.open(fmri_file_path, 'rb') as f_in:\n",
    "            fmri_img = nib.FileHolder(fileobj=f_in)\n",
    "            fmri_data = nib.Nifti1Image.from_file_map({'image': fmri_img}).get_fdata(dtype=np.float32)\n",
    "    else:\n",
    "        fmri_data = nib.load(fmri_file_path).get_fdata(dtype=np.float32)\n",
    "\n",
    "    fmri_data = (fmri_data - fmri_data.min()) / (fmri_data.max() - fmri_data.min() + 1e-10)\n",
    "    total_frames = fmri_data.shape[-1]\n",
    "\n",
    "    num_sequences = (total_frames - sequence_length - future_steps) // batch_size + 1\n",
    "    sequences = np.zeros((num_sequences * batch_size, sequence_length, *target_shape), dtype=np.float32)\n",
    "    sequence_labels = []\n",
    "\n",
    "    idx = 0\n",
    "    for i in range(0, total_frames - sequence_length - future_steps, batch_size):\n",
    "        batch_sequences = fmri_data[..., i:i + batch_size + sequence_length + future_steps]\n",
    "        for j in range(min(batch_size, total_frames - i - sequence_length - future_steps)):\n",
    "            seq = batch_sequences[..., j:j + sequence_length]\n",
    "            seq = resize(seq, (*target_shape, sequence_length), anti_aliasing=True, preserve_range=True)\n",
    "            sequences[idx] = np.transpose(seq, (3, 0, 1, 2))  # (sequence_length, 32, 32, 32)\n",
    "            sequence_labels.append(labels[min(i + j + sequence_length, len(labels) - 1)])\n",
    "            idx += 1\n",
    "\n",
    "    if idx > 0:\n",
    "        return sequences[:idx], sequence_labels\n",
    "    return None, None\n",
    "\n",
    "# Build Transformer model\n",
    "def build_transformer_model(sequence_length, target_shape, num_classes, future_steps, d_model=128, num_heads=4, ff_dim=256, dropout_rate=0.1):\n",
    "    inputs = layers.Input(shape=(sequence_length, *target_shape, 1))\n",
    "\n",
    "    x = layers.TimeDistributed(layers.Conv3D(16, (3, 3, 3), activation='relu', padding='same', strides=2))(inputs)\n",
    "    x = layers.TimeDistributed(layers.GlobalAveragePooling3D())(x)\n",
    "    x = layers.Dense(d_model)(x)\n",
    "\n",
    "    positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "    pos_encoding = layers.Embedding(input_dim=sequence_length, output_dim=d_model)(positions)\n",
    "    x = x + pos_encoding\n",
    "\n",
    "    for _ in range(2):  # Two Transformer blocks\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        ffn_output = layers.Dense(ff_dim, activation='relu')(x)\n",
    "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
    "        ffn_output = layers.Dense(d_model)(ffn_output)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(future_steps * num_classes, activation='linear')(x)\n",
    "    outputs = layers.Reshape((future_steps, num_classes))(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Main Execution\n",
    "label_mapping = load_labels_from_tsv(label_path)\n",
    "\n",
    "fmri_sequences, label_sequences = [], []\n",
    "for root, _, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith(('.nii', '.nii.gz')):\n",
    "            file_path = os.path.join(root, file)\n",
    "            seqs, labels = process_fmri_file_sequence(file_path, label_mapping)\n",
    "            if seqs is not None and labels:\n",
    "                fmri_sequences.append(seqs)\n",
    "                label_sequences.extend(labels)\n",
    "\n",
    "if not fmri_sequences:\n",
    "    raise ValueError(\"No fMRI sequences processed!\")\n",
    "\n",
    "fmri_sequences = np.concatenate(fmri_sequences, axis=0)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(label_sequences)\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "labels_one_hot = one_hot_encoder.fit_transform(encoded_labels.reshape(-1, 1))\n",
    "\n",
    "# Prepare future label sequences\n",
    "y_future = np.zeros((len(labels_one_hot), future_steps, labels_one_hot.shape[1]))\n",
    "for i in range(len(labels_one_hot) - future_steps):\n",
    "    y_future[i] = labels_one_hot[i:i + future_steps]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    fmri_sequences, y_future, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# Build and train model\n",
    "model = build_transformer_model(sequence_length, target_shape, labels_one_hot.shape[1], future_steps)\n",
    "\n",
    "# Optional: Visualize the model\n",
    "plot_model(model, show_shapes=True, to_file=\"transformer_model.png\")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, callbacks=callbacks, verbose=1)\n",
    "\n",
    "# Evaluation\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Prediction samples\n",
    "predictions = model.predict(X_test, verbose=0)\n",
    "for i in range(min(5, len(X_test))):\n",
    "    predicted_labels = label_encoder.inverse_transform(np.argmax(predictions[i], axis=1))\n",
    "    actual_labels = label_encoder.inverse_transform(np.argmax(y_test[i], axis=1))\n",
    "    print(f\"Sample {i + 1} - Actual: {actual_labels}, Predicted: {predicted_labels}\")\n",
    "\n",
    "# Save the model and encoders\n",
    "model.save(\"fmri_emotion_model_transformer.keras\")\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"Transformer model and label encoder saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65bb6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
